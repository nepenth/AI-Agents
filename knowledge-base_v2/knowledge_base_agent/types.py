from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Any

from pydantic import BaseModel, Field, HttpUrl, field_validator, model_validator

import logging
logger = logging.getLogger(__name__)


class MediaItem(BaseModel):
    """Represents a single media item associated with a tweet."""
    original_url: HttpUrl
    local_path: Optional[Path] = None # Path where the media is cached
    type: Optional[str] = None # e.g., 'image', 'video', 'gif' - auto-detected or from source
    alt_text: Optional[str] = None # Alt text from Twitter/X, if available
    description: Optional[str] = None # Generated by vision model
    download_error: Optional[str] = None  # Error message if download failed
    interpret_error: Optional[str] = None # Error message if interpretation failed

    @field_validator("local_path", mode="before")
    @classmethod
    def validate_path(cls, v):
        if v is not None and not isinstance(v, Path):
            return Path(v)
        return v


class TweetData(BaseModel):
    """
    Core data structure representing a tweet and its processing state.
    This model serves as the primary record in the tweet_cache.
    """
    tweet_id: str = Field(..., description="The unique identifier of the tweet.")
    author_handle: Optional[str] = None
    author_id: Optional[str] = None # Add author ID for thread checking
    author_name: Optional[str] = None
    created_at: Optional[datetime] = None
    text: Optional[str] = None
    thread_tweets: List[Dict[str, Any]] = Field(default_factory=list, description="List of subsequent thread tweet dicts by the same author.") # Store basic info
    combined_text: Optional[str] = None # Combined text of main tweet + thread
    source_url: Optional[HttpUrl] = Field(None, description="The direct URL to the tweet.")
    original_urls: List[str] = Field(default_factory=list, description="Original t.co URLs extracted from tweet text.")
    expanded_urls: Dict[str, HttpUrl] = Field(default_factory=dict, description="Mapping of shortened URLs to their expanded forms.")
    media_items: List[MediaItem] = Field(default_factory=list)
    referenced_tweets: List[str] = Field(default_factory=list, description="IDs of tweets referenced (e.g., quotes, replies)") # Future use?

    # --- ADDED --- Raw data from twscrape
    raw_tweet_details: Optional[Dict[str, Any]] = Field(None, description="Raw tweet details dict from twscrape, if available.")

    # Processing Status Flags
    cache_complete: bool = Field(default=False, description="Phase 1: Core data and media files cached/validated.")
    media_processed: bool = Field(default=False, description="Phase 2: Media interpretation (e.g., descriptions) complete.")
    categories_processed: bool = Field(default=False, description="Phase 3: Categories and item name assigned.")
    kb_item_created: bool = Field(default=False, description="Phase 4: Knowledge base item (README, media) generated.")
    db_synced: bool = Field(default=False, description="Phase 5: Entry synced with the local SQLite DB.")

    # Timestamps
    last_cached_at: Optional[datetime] = Field(None, description="Timestamp of the last successful caching attempt for this tweet.")
    last_interpreted_at: Optional[datetime] = Field(None, description="Timestamp of the last successful media interpretation attempt.")

    # Generated Content/Metadata
    main_category: Optional[str] = None
    sub_category: Optional[str] = None
    item_name: Optional[str] = None # Generated name for the KB item directory/entry
    generated_content: Optional[str] = None # The generated README content (Phase 4)

    # Knowledge Base Output Info
    kb_item_path: Optional[Path] = Field(None, description="Path to the generated KB item directory.")
    kb_media_paths: List[Path] = Field(default_factory=list, description="Paths to media files copied into the KB item directory.")

    # Error Tracking
    error_message: Optional[str] = None # Record the first error encountered during processing
    failed_phase: Optional[str] = None # Record the name of the phase that failed
    cache_error: Optional[str] = None # <<<< ADDED
    media_interpret_error: Optional[str] = None # <<<< ADDED (or keep commented in mark_failed)
    categorization_error: Optional[str] = None # <<<< ADDED (or keep commented in mark_failed)
    generation_error: Optional[str] = None # <<<< ADDED (or keep commented in mark_failed)
    dbsync_error: Optional[str] = None # <<<< ADDED (or keep commented in mark_failed)

    @field_validator("kb_item_path", mode="before")
    @classmethod
    def validate_kb_item_path(cls, v):
         # Allow None, otherwise convert to Path
         if v is not None and not isinstance(v, Path):
             return Path(v)
         return v

    @field_validator("kb_media_paths", mode="before")
    @classmethod
    def validate_kb_media_paths(cls, v):
        # Ensure list contains Path objects
        if isinstance(v, list):
             return [Path(p) if not isinstance(p, Path) else p for p in v]
        return v # Allow empty list or potentially None if default_factory wasn't used

    @model_validator(mode='after')
    def compute_combined_text(self) -> "TweetData":
        """Combines the main text and thread tweet texts."""
        if not self.text and not self.thread_tweets:
             self.combined_text = None
             return self

        all_texts = []
        if self.text:
             all_texts.append(self.text)

        # Sort thread tweets chronologically before combining
        sorted_threads = sorted(self.thread_tweets, key=lambda t: t.get('created_at', datetime.min))

        for i, thread_tweet in enumerate(sorted_threads):
             thread_text = thread_tweet.get('text')
             if thread_text:
                  all_texts.append(f"\n\n--- (Thread Part {i+1}) ---\n{thread_text}") # Add separator

        self.combined_text = "".join(all_texts)
        return self

    def mark_failed(self, phase: str, error: Exception | str):
        """Updates the status to reflect a processing failure."""
        self.failed_phase = phase
        error_str = str(error)
        self.error_message = error_str # Store general error
        
        # Set phase-specific error flags if they exist
        phase_lower = phase.lower()
        if phase_lower in ["caching", "cache"]:
            self.cache_complete = False
            self.cache_error = error_str # Use the added field
        elif phase_lower in ["interpretation", "interpret"]:
            self.media_processed = False
            self.media_interpret_error = error_str # Use the added field
        elif phase_lower in ["categorization", "categorize"]:
            self.categories_processed = False
            self.categorization_error = error_str # Use the added field
        elif phase_lower in ["generation", "generate"]:
            self.kb_item_created = False
            self.generation_error = error_str # Use the added field
        elif phase_lower in ["dbsync", "sync", "database", "db_sync"]:
            self.db_synced = False
            self.dbsync_error = error_str # Use the added field


    def needs_processing(self, phase: str) -> bool:
        """
        Checks if a specific processing phase needs to run for this tweet based on its current state.
        This method does NOT consider external 'force' flags.
        It allows a phase to re-run if it previously failed (i.e., failed_phase matches current phase).
        """
        phase_lower = phase.lower()

        # If a different phase previously failed, and it wasn't this one, don't proceed.
        if self.failed_phase and self.failed_phase.lower() != phase_lower:
            # Allow first phase (e.g. caching) to run even if there was a previous generic error from a prior run
            # This assumes phases are somewhat ordered. If caching hasn't run, it should.
            if phase_lower not in ["caching", "cache", "fetcher", "fetch"]: # Adjust first phases as needed
                 logger.debug(f"Tweet {self.tweet_id}: Skipping phase '{phase}' because prior phase '{self.failed_phase}' failed.")
                 return False
        
        # If this specific phase previously failed, it needs processing (retry)
        if self.failed_phase and self.failed_phase.lower() == phase_lower:
            logger.debug(f"Tweet {self.tweet_id}: Phase '{phase}' needs processing due to previous failure in this phase.")
            return True

        # Standard checks for phase completion
        if phase_lower in ["caching", "cache"]:
            return not self.cache_complete
        elif phase_lower in ["interpretation", "interpret"]:
            # Requires cache complete. Media not processed.
            if not self.cache_complete: return False
            # Optional: check if there's actually media that needs interpreting
            # has_uninterpreted_images = any(m.type == 'image' and not m.description for m in self.media_items)
            # return not self.media_processed and has_uninterpreted_images
            return not self.media_processed
        elif phase_lower in ["categorization", "categorize"]:
            if not self.cache_complete or not self.media_processed: return False
            return not self.categories_processed
        elif phase_lower in ["generation", "generate"]:
            if not self.cache_complete or not self.media_processed or not self.categories_processed: return False
            return not self.kb_item_created
        elif phase_lower in ["dbsync", "sync", "database", "db_sync"]:
            if not self.kb_item_created: return False
            return not self.db_synced
        else:
            logger.warning(f"Tweet {self.tweet_id}: Unknown phase '{phase}' requested in needs_processing.")
            return False


class ProcessingState(BaseModel):
    """Represents the overall state managed by the StateManager."""
    unprocessed_tweet_ids: Set[str] = Field(default_factory=set)
    processed_tweet_ids: Set[str] = Field(default_factory=set)
    # Using dict allows easy lookup by ID. Key is tweet_id.
    tweet_cache: Dict[str, TweetData] = Field(default_factory=dict)

    # Metadata or statistics could also be added here
    last_run_timestamp: Optional[datetime] = None


# --- Database Related Types ---
# Defines the structure expected for interacting with the DB layer.
# The actual SQLAlchemy model might differ slightly (e.g., relationships).

class KnowledgeBaseItemRecord(BaseModel):
    """Pydantic model representing a record in the knowledge base summary DB."""
    # Note: Primary key (id) is often handled by SQLAlchemy automatically
    tweet_id: str = Field(..., description="Original tweet ID.", index=True) # Indexing likely needed
    item_name: str = Field(...)
    main_category: str = Field(...)
    sub_category: str = Field(...)
    kb_item_path: Path = Field(..., description="Path relative to KNOWLEDGE_BASE_DIR")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    last_updated_at: datetime = Field(default_factory=datetime.utcnow)
    source_url: Optional[HttpUrl] = None
    author_handle: Optional[str] = None # Denormalized for easier display?
    text_preview: Optional[str] = None # Denormalized for easier display?

    @field_validator("kb_item_path", mode="before")
    @classmethod
    def validate_path(cls, v):
         if not isinstance(v, Path):
             return Path(v)
         return v

    # Pydantic configuration for ORM mode if needed later
    # model_config = ConfigDict(from_attributes=True)
