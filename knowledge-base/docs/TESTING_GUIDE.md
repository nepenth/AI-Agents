# JSON Prompt System Testing Guide\n\n## Overview\n\nThis guide covers comprehensive testing strategies for the JSON Prompt System, including unit tests, integration tests, performance testing, and comparison testing with the original system.\n\n## Test Structure\n\nThe testing framework consists of several components:\n\n```\ntests/\n├── test_json_prompt.py              # JsonPrompt class unit tests\n├── test_json_prompt_manager.py      # JsonPromptManager unit tests\n└── test_integration.py              # Integration tests\n\nknowledge_base_agent/\n├── test_prompt_comparison.py        # Original vs JSON comparison tests\n├── test_performance_quality.py     # Performance and quality tests\n├── test_integration.py              # System integration tests\n├── cli_prompt_tools.py              # CLI testing tools\n└── automated_test_runner.py         # Automated test execution\n```\n\n## Running Tests\n\n### Quick Test Commands\n\n```bash\n# Run all unit tests\npython -m pytest tests/ -v\n\n# Run specific test files\npython -m pytest tests/test_json_prompt.py -v\npython -m pytest tests/test_json_prompt_manager.py -v\npython -m pytest tests/test_integration.py -v\n\n# Run with coverage\npython -m pytest tests/ --cov=knowledge_base_agent --cov-report=html\n\n# Run comparison tests\npython knowledge_base_agent/test_prompt_comparison.py\n\n# Run performance tests\npython knowledge_base_agent/test_performance_quality.py\n\n# Run integration tests\npython knowledge_base_agent/test_integration.py\n```\n\n### CLI Testing Tools\n\n```bash\n# Validate all prompts\npython knowledge_base_agent/cli_prompt_tools.py validate-all\n\n# Test specific prompt\npython knowledge_base_agent/cli_prompt_tools.py test-prompt categorization_standard\n\n# Compare prompt outputs\npython knowledge_base_agent/cli_prompt_tools.py compare-prompts categorization_standard\n\n# Run performance benchmarks\npython knowledge_base_agent/cli_prompt_tools.py benchmark\n```\n\n### Automated Test Runner\n\n```bash\n# Run comprehensive test suite\npython knowledge_base_agent/automated_test_runner.py\n\n# Run specific test categories\npython knowledge_base_agent/automated_test_runner.py --unit-tests\npython knowledge_base_agent/automated_test_runner.py --integration-tests\npython knowledge_base_agent/automated_test_runner.py --performance-tests\npython knowledge_base_agent/automated_test_runner.py --comparison-tests\n```\n\n## Unit Testing\n\n### JsonPrompt Class Tests\n\nLocation: `tests/test_json_prompt.py`\n\n#### Test Categories\n\n1. **Basic Functionality**\n   - Prompt loading from files and dictionaries\n   - Property accessors\n   - Error handling for invalid inputs\n\n2. **Validation**\n   - Schema validation\n   - Parameter validation\n   - Template structure validation\n\n3. **Parameter Processing**\n   - Required vs optional parameters\n   - Type validation\n   - Default value handling\n   - Validation rules (length, enum, etc.)\n\n4. **Template Rendering**\n   - Standard model rendering\n   - Reasoning model rendering\n   - Parameter substitution\n   - Error handling\n\n5. **Variants and Conditional Logic**\n   - Variant selection\n   - Condition evaluation\n   - Template switching\n\n#### Example Test Cases\n\n```python\ndef test_basic_prompt_loading():\n    \"\"\"Test loading prompts from various sources.\"\"\"\n    # Test loading from dictionary\n    prompt_data = {\n        \"prompt_id\": \"test_prompt\",\n        \"prompt_name\": \"Test Prompt\",\n        \"description\": \"A test prompt\",\n        \"model_type\": \"standard\",\n        \"category\": \"test\",\n        \"task\": \"test\",\n        \"input_parameters\": {\"required\": [], \"parameters\": {}},\n        \"template\": {\"type\": \"standard\", \"content\": \"test\"}\n    }\n    \n    prompt = JsonPrompt(prompt_data)\n    assert prompt.prompt_id == \"test_prompt\"\n    assert prompt.model_type == \"standard\"\n\ndef test_parameter_validation():\n    \"\"\"Test parameter validation functionality.\"\"\"\n    prompt_data = {\n        # ... prompt definition with validation rules\n        \"input_parameters\": {\n            \"required\": [\"text\"],\n            \"parameters\": {\n                \"text\": {\n                    \"type\": \"string\",\n                    \"validation\": {\"min_length\": 5, \"max_length\": 100}\n                }\n            }\n        }\n    }\n    \n    prompt = JsonPrompt(prompt_data)\n    \n    # Test valid parameters\n    errors = prompt.validate_parameters({\"text\": \"Valid text\"})\n    assert len(errors) == 0\n    \n    # Test invalid parameters\n    errors = prompt.validate_parameters({\"text\": \"Hi\"})  # Too short\n    assert len(errors) > 0\n    assert \"must be at least 5 characters\" in errors[0]\n\ndef test_template_rendering():\n    \"\"\"Test template rendering with parameters.\"\"\"\n    prompt_data = {\n        # ... prompt definition\n        \"template\": {\n            \"type\": \"standard\",\n            \"content\": \"Hello {{name}}! Welcome to {{place}}.\"\n        }\n    }\n    \n    prompt = JsonPrompt(prompt_data)\n    result = prompt.render({\"name\": \"Alice\", \"place\": \"Wonderland\"})\n    \n    assert \"Hello Alice! Welcome to Wonderland.\" in result.content\n    assert result.model_type == \"standard\"\n    assert result.render_time_ms > 0\n```\n\n### JsonPromptManager Class Tests\n\nLocation: `tests/test_json_prompt_manager.py`\n\n#### Test Categories\n\n1. **Basic Operations**\n   - Manager initialization\n   - Prompt loading and caching\n   - Available prompts discovery\n\n2. **Caching System**\n   - Cache hits and misses\n   - File modification detection\n   - Cache clearing\n   - Performance metrics\n\n3. **Prompt Rendering**\n   - Standard and reasoning model rendering\n   - Parameter validation integration\n   - Error handling\n\n4. **Search and Discovery**\n   - Prompt searching by name/description\n   - Category-based filtering\n   - Metadata queries\n\n5. **Validation**\n   - Individual prompt validation\n   - Batch validation\n   - Schema compliance\n\n#### Example Test Cases\n\n```python\ndef test_prompt_caching():\n    \"\"\"Test prompt caching functionality.\"\"\"\n    manager = JsonPromptManager(prompts_dir=test_prompts_dir)\n    \n    # First load (cache miss)\n    prompt1 = manager.load_prompt(\"test_prompt\", \"standard\")\n    stats1 = manager.get_cache_stats()\n    \n    # Second load (cache hit)\n    prompt2 = manager.load_prompt(\"test_prompt\", \"standard\")\n    stats2 = manager.get_cache_stats()\n    \n    # Should be the same object from cache\n    assert prompt1 is prompt2\n    assert stats2[\"cache_hits\"] == stats1[\"cache_hits\"] + 1\n\ndef test_prompt_search():\n    \"\"\"Test prompt search functionality.\"\"\"\n    manager = JsonPromptManager(prompts_dir=test_prompts_dir)\n    \n    # Search by name\n    results = manager.search_prompts(\"categorization\")\n    assert len(results) > 0\n    assert any(\"categorization\" in r[\"prompt_name\"].lower() for r in results)\n    \n    # Search by category\n    cat_results = manager.get_prompts_by_category(\"chat\")\n    assert all(r[\"category\"] == \"chat\" for r in cat_results)\n\ndef test_validation():\n    \"\"\"Test prompt validation.\"\"\"\n    manager = JsonPromptManager(prompts_dir=test_prompts_dir)\n    \n    # Validate existing prompt\n    result = manager.validate_prompt(\"test_prompt\", \"standard\")\n    assert result[\"valid\"] == True\n    assert len(result[\"errors\"]) == 0\n    \n    # Test validation with examples\n    if result[\"examples_valid\"]:\n        assert result[\"example_results\"] is not None\n```\n\n## Integration Testing\n\n### System Integration Tests\n\nLocation: `tests/test_integration.py` and `knowledge_base_agent/test_integration.py`\n\n#### Test Categories\n\n1. **Real Prompt File Testing**\n   - Loading all actual prompt files\n   - Validation of production prompts\n   - Cross-prompt consistency\n\n2. **End-to-End Workflows**\n   - Complete categorization workflow\n   - KB item generation workflow\n   - Chat interaction workflow\n\n3. **System Integration**\n   - Integration with Config objects\n   - Environment variable handling\n   - Error recovery scenarios\n\n#### Example Integration Tests\n\n```python\ndef test_categorization_workflow():\n    \"\"\"Test complete categorization workflow.\"\"\"\n    manager = JsonPromptManager()\n    \n    # Test standard categorization\n    result = manager.render_prompt(\n        \"categorization_standard\",\n        {\n            \"context_content\": \"Advanced React hooks patterns for state management\",\n            \"formatted_existing_categories\": \"frontend_frameworks, react_patterns, state_management\",\n            \"is_thread\": False\n        },\n        \"standard\"\n    )\n    \n    assert result.model_type == \"standard\"\n    assert \"React\" in result.content\n    assert \"categorize\" in result.content.lower()\n    \n    # Test reasoning categorization\n    reasoning_result = manager.render_prompt(\n        \"categorization_reasoning\",\n        {\n            \"context_content\": \"Thread about microservices architecture patterns\",\n            \"formatted_existing_categories\": \"microservices, architecture, distributed_systems\",\n            \"is_thread\": True\n        },\n        \"reasoning\"\n    )\n    \n    assert reasoning_result.model_type == \"reasoning\"\n    assert isinstance(reasoning_result.content, dict)\n    assert \"system\" in reasoning_result.content\n    assert \"user\" in reasoning_result.content\n\ndef test_kb_item_generation_workflow():\n    \"\"\"Test KB item generation workflow.\"\"\"\n    manager = JsonPromptManager()\n    \n    context_data = {\n        'tweet_segments': [\n            {'text': 'React hooks are powerful', 'order': 1},\n            {'text': 'They enable functional components', 'order': 2}\n        ],\n        'tweet_text': 'React hooks are powerful. They enable functional components to have state.',\n        'media_descriptions': ['Code example showing useState hook'],\n        'urls': ['https://reactjs.org/docs/hooks-intro.html'],\n        'is_thread': False\n    }\n    \n    result = manager.render_prompt(\n        \"kb_item_generation_standard\",\n        context_data,\n        \"standard\"\n    )\n    \n    assert \"React hooks\" in result.content\n    assert \"JSON\" in result.content  # Should request JSON output\n    assert len(result.content) > 500  # Should be substantial\n```\n\n## Comparison Testing\n\n### Original vs JSON Prompt Comparison\n\nLocation: `knowledge_base_agent/test_prompt_comparison.py`\n\n#### Test Strategy\n\n1. **Output Similarity Testing**\n   - Compare rendered outputs between systems\n   - Use similarity metrics (cosine similarity, edit distance)\n   - Allow for acceptable variations\n\n2. **Functional Equivalence**\n   - Test identical inputs produce similar results\n   - Verify key information is preserved\n   - Check output format consistency\n\n3. **Edge Case Testing**\n   - Test with boundary conditions\n   - Error handling comparison\n   - Performance under stress\n\n#### Example Comparison Tests\n\n```python\ndef test_categorization_similarity():\n    \"\"\"Compare categorization prompts between systems.\"\"\"\n    # Test parameters\n    context_content = \"Advanced React hooks patterns for state management\"\n    formatted_categories = \"frontend_frameworks, react_patterns, state_management\"\n    is_thread = False\n    \n    # Get original prompt\n    from knowledge_base_agent.prompts import LLMPrompts as OriginalLLMPrompts\n    original_prompt = OriginalLLMPrompts.get_categorization_prompt_standard(\n        context_content, formatted_categories, is_thread\n    )\n    \n    # Get JSON prompt\n    manager = JsonPromptManager()\n    json_result = manager.render_prompt(\n        \"categorization_standard\",\n        {\n            \"context_content\": context_content,\n            \"formatted_existing_categories\": formatted_categories,\n            \"is_thread\": is_thread\n        },\n        \"standard\"\n    )\n    \n    # Compare similarity\n    similarity = calculate_similarity(original_prompt, json_result.content)\n    assert similarity > 0.90, f\"Similarity too low: {similarity:.2%}\"\n    \n    # Check key elements are present\n    key_elements = [\"categorize\", \"JSON\", \"main_category\", \"sub_category\"]\n    for element in key_elements:\n        assert element in original_prompt\n        assert element in json_result.content\n\ndef calculate_similarity(text1: str, text2: str) -> float:\n    \"\"\"Calculate similarity between two texts.\"\"\"\n    from difflib import SequenceMatcher\n    return SequenceMatcher(None, text1, text2).ratio()\n```\n\n## Performance Testing\n\n### Performance and Quality Tests\n\nLocation: `knowledge_base_agent/test_performance_quality.py`\n\n#### Test Categories\n\n1. **Rendering Performance**\n   - Prompt rendering speed\n   - Memory usage during rendering\n   - Cache performance impact\n\n2. **Loading Performance**\n   - Prompt loading times\n   - Manager initialization speed\n   - File system performance\n\n3. **Scalability Testing**\n   - Performance with many prompts\n   - Concurrent access testing\n   - Memory usage under load\n\n4. **Quality Metrics**\n   - Prompt completeness\n   - Template complexity\n   - Parameter coverage\n\n#### Example Performance Tests\n\n```python\ndef test_rendering_performance():\n    \"\"\"Test prompt rendering performance.\"\"\"\n    manager = JsonPromptManager()\n    \n    # Warm up cache\n    manager.render_prompt(\"chat_standard\", {}, \"standard\")\n    \n    # Measure rendering time\n    start_time = time.time()\n    for _ in range(100):\n        result = manager.render_prompt(\"chat_standard\", {}, \"standard\")\n    end_time = time.time()\n    \n    avg_time = (end_time - start_time) / 100\n    assert avg_time < 0.01, f\"Rendering too slow: {avg_time:.4f}s per render\"\n\ndef test_memory_usage():\n    \"\"\"Test memory usage during prompt operations.\"\"\"\n    import psutil\n    import os\n    \n    process = psutil.Process(os.getpid())\n    \n    # Baseline memory\n    baseline_memory = process.memory_info().rss / 1024 / 1024\n    \n    # Load many prompts\n    manager = JsonPromptManager()\n    available_prompts = manager.get_available_prompts()\n    \n    for model_type, prompt_ids in available_prompts.items():\n        for prompt_id in prompt_ids:\n            manager.load_prompt(prompt_id, model_type)\n    \n    # Check memory usage\n    final_memory = process.memory_info().rss / 1024 / 1024\n    memory_increase = final_memory - baseline_memory\n    \n    # Should not use excessive memory\n    assert memory_increase < 50, f\"Memory usage too high: {memory_increase:.2f} MB\"\n\ndef test_concurrent_access():\n    \"\"\"Test concurrent access to prompt manager.\"\"\"\n    import threading\n    import time\n    \n    manager = JsonPromptManager()\n    results = []\n    errors = []\n    \n    def render_prompt_worker():\n        try:\n            for _ in range(10):\n                result = manager.render_prompt(\"chat_standard\", {}, \"standard\")\n                results.append(len(result.content))\n                time.sleep(0.01)\n        except Exception as e:\n            errors.append(str(e))\n    \n    # Start multiple threads\n    threads = []\n    for _ in range(5):\n        thread = threading.Thread(target=render_prompt_worker)\n        threads.append(thread)\n        thread.start()\n    \n    # Wait for completion\n    for thread in threads:\n        thread.join()\n    \n    # Check results\n    assert len(errors) == 0, f\"Concurrent access errors: {errors}\"\n    assert len(results) == 50, f\"Expected 50 results, got {len(results)}\"\n    assert all(r > 0 for r in results), \"All results should be non-empty\"\n```\n\n## Test Data Management\n\n### Test Prompt Creation\n\n```python\ndef create_test_prompt(prompt_id: str, model_type: str = \"standard\") -> Dict[str, Any]:\n    \"\"\"Create a test prompt for testing purposes.\"\"\"\n    return {\n        \"prompt_id\": prompt_id,\n        \"prompt_name\": f\"Test {prompt_id.title()}\",\n        \"description\": f\"Test prompt for {prompt_id}\",\n        \"model_type\": model_type,\n        \"category\": \"test\",\n        \"task\": \"test task\",\n        \"input_parameters\": {\n            \"required\": [\"input\"],\n            \"optional\": [\"optional_param\"],\n            \"parameters\": {\n                \"input\": {\n                    \"type\": \"string\",\n                    \"description\": \"Test input parameter\"\n                },\n                \"optional_param\": {\n                    \"type\": \"string\",\n                    \"description\": \"Optional test parameter\",\n                    \"default\": \"default_value\"\n                }\n            }\n        },\n        \"template\": {\n            \"type\": model_type,\n            \"content\": \"Test template with {{input}} and {{optional_param}}\" if model_type == \"standard\" else None,\n            \"system_message\": \"Test system message with {{input}}\" if model_type == \"reasoning\" else None,\n            \"user_message\": \"Test user message with {{optional_param}}\" if model_type == \"reasoning\" else None\n        },\n        \"examples\": [\n            {\n                \"name\": \"basic_example\",\n                \"input\": {\"input\": \"test input\"},\n                \"expected_output\": \"Test template with test input and default_value\"\n            }\n        ]\n    }\n```\n\n### Test Environment Setup\n\n```python\ndef setup_test_environment():\n    \"\"\"Set up test environment with temporary prompt directory.\"\"\"\n    import tempfile\n    import json\n    from pathlib import Path\n    \n    # Create temporary directory\n    temp_dir = Path(tempfile.mkdtemp())\n    prompts_dir = temp_dir / \"prompts_json\"\n    prompts_dir.mkdir()\n    \n    # Create standard and reasoning directories\n    (prompts_dir / \"standard\").mkdir()\n    (prompts_dir / \"reasoning\").mkdir()\n    \n    # Create config file\n    config_data = {\n        \"schema_version\": \"1.0.0\",\n        \"description\": \"Test configuration\",\n        \"prompt_directories\": {\n            \"standard\": {\"path\": \"standard/\", \"model_type\": \"standard\"},\n            \"reasoning\": {\"path\": \"reasoning/\", \"model_type\": \"reasoning\"}\n        }\n    }\n    \n    with open(prompts_dir / \"config.json\", 'w') as f:\n        json.dump(config_data, f)\n    \n    # Create test prompts\n    test_prompts = {\n        \"standard\": [\"test_standard\", \"chat_test\", \"categorization_test\"],\n        \"reasoning\": [\"test_reasoning\", \"system_test\"]\n    }\n    \n    for model_type, prompt_ids in test_prompts.items():\n        for prompt_id in prompt_ids:\n            prompt_data = create_test_prompt(prompt_id, model_type)\n            prompt_file = prompts_dir / model_type / f\"{prompt_id}.json\"\n            \n            with open(prompt_file, 'w') as f:\n                json.dump(prompt_data, f, indent=2)\n    \n    return prompts_dir\n```\n\n## Continuous Integration\n\n### GitHub Actions Configuration\n\n```yaml\n# .github/workflows/test.yml\nname: Test JSON Prompt System\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.8, 3.9, '3.10', 3.11]\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v3\n      with:\n        python-version: ${{ matrix.python-version }}\n    \n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n        pip install pytest pytest-cov\n    \n    - name: Run unit tests\n      run: |\n        python -m pytest tests/ -v --cov=knowledge_base_agent --cov-report=xml\n    \n    - name: Run integration tests\n      run: |\n        python knowledge_base_agent/test_integration.py\n    \n    - name: Run comparison tests\n      run: |\n        python knowledge_base_agent/test_prompt_comparison.py\n    \n    - name: Run performance tests\n      run: |\n        python knowledge_base_agent/test_performance_quality.py\n    \n    - name: Upload coverage to Codecov\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n```\n\n### Pre-commit Hooks\n\n```yaml\n# .pre-commit-config.yaml\nrepos:\n  - repo: local\n    hooks:\n      - id: prompt-validation\n        name: Validate JSON Prompts\n        entry: python knowledge_base_agent/cli_prompt_tools.py validate-all\n        language: system\n        pass_filenames: false\n        \n      - id: prompt-tests\n        name: Run Prompt Tests\n        entry: python -m pytest tests/test_json_prompt.py tests/test_json_prompt_manager.py -v\n        language: system\n        pass_filenames: false\n```\n\n## Test Reporting\n\n### Coverage Reports\n\n```bash\n# Generate HTML coverage report\npython -m pytest tests/ --cov=knowledge_base_agent --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html\n```\n\n### Performance Reports\n\n```python\ndef generate_performance_report():\n    \"\"\"Generate comprehensive performance report.\"\"\"\n    from knowledge_base_agent.test_performance_quality import PerformanceQualityTester\n    \n    tester = PerformanceQualityTester()\n    results = tester.run_performance_tests()\n    \n    report = {\n        \"timestamp\": datetime.now().isoformat(),\n        \"performance_metrics\": {\n            \"average_render_time_ms\": results[\"average_render_time_ms\"],\n            \"memory_usage_mb\": results[\"memory_usage_mb\"],\n            \"cache_hit_rate\": results[\"cache_hit_rate\"],\n            \"prompts_per_second\": results[\"prompts_per_second\"]\n        },\n        \"quality_metrics\": {\n            \"total_prompts\": results[\"total_prompts\"],\n            \"valid_prompts\": results[\"valid_prompts\"],\n            \"validation_pass_rate\": results[\"validation_pass_rate\"]\n        }\n    }\n    \n    with open(\"performance_report.json\", \"w\") as f:\n        json.dump(report, f, indent=2)\n    \n    return report\n```\n\n## Best Practices\n\n### Test Organization\n\n1. **Separate Concerns**: Keep unit tests, integration tests, and performance tests separate\n2. **Use Fixtures**: Create reusable test fixtures for common setup\n3. **Mock External Dependencies**: Mock file system, network calls, etc.\n4. **Test Edge Cases**: Include boundary conditions and error scenarios\n5. **Maintain Test Data**: Keep test prompts simple but representative\n\n### Test Maintenance\n\n1. **Regular Updates**: Update tests when prompts change\n2. **Performance Baselines**: Establish and maintain performance baselines\n3. **Coverage Monitoring**: Maintain high test coverage (>90%)\n4. **Automated Execution**: Run tests automatically on code changes\n5. **Documentation**: Keep test documentation up to date\n\n### Debugging Tests\n\n```python\n# Enable verbose logging for debugging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Use pytest debugging features\npython -m pytest tests/test_json_prompt.py::test_specific_function -v -s --pdb\n\n# Print intermediate values\ndef test_with_debugging():\n    prompt = JsonPrompt(test_data)\n    print(f\"Loaded prompt: {prompt.prompt_id}\")\n    \n    result = prompt.render(test_params)\n    print(f\"Rendered content length: {len(result.content)}\")\n    print(f\"First 100 chars: {result.content[:100]}\")\n    \n    assert len(result.content) > 0\n```\n\n## Conclusion\n\nThe JSON Prompt System includes comprehensive testing at multiple levels:\n\n- **Unit Tests**: Thorough testing of individual components\n- **Integration Tests**: End-to-end workflow validation\n- **Comparison Tests**: Equivalence with original system\n- **Performance Tests**: Speed and resource usage validation\n- **Automated Testing**: Continuous integration and validation\n\nThis testing framework ensures the reliability, performance, and correctness of the JSON prompt system while maintaining compatibility with the original implementation.\n"