# Prompts Replacement Module Usage Guide\n\n## Overview\n\nThe `prompts_replacement.py` module provides a drop-in replacement for the original `prompts.py` module, allowing seamless migration to the JSON-based prompt system while maintaining identical interfaces.\n\n## Quick Start\n\n### Basic Usage\n\n```python\n# Import exactly like the original prompts module\nfrom knowledge_base_agent.prompts_replacement import LLMPrompts, ReasoningPrompts\n\n# Use identical method signatures\nchat_prompt = LLMPrompts.get_chat_prompt()\ncategorization_prompt = LLMPrompts.get_categorization_prompt_standard(\n    \"React hooks tutorial\",\n    \"frontend, react, hooks\", \n    False\n)\n\nsystem_message = ReasoningPrompts.get_system_message()\n```\n\n### Environment Control\n\n```python\nfrom knowledge_base_agent.prompts_replacement import (\n    use_json_prompts, \n    is_using_json_prompts, \n    get_prompt_system_info\n)\n\n# Enable JSON prompts (default)\nuse_json_prompts(True)\n\n# Check current status\nprint(f\"Using JSON prompts: {is_using_json_prompts()}\")\n\n# Get detailed system information\ninfo = get_prompt_system_info()\nprint(f\"System info: {info}\")\n\n# Disable JSON prompts (fallback to original)\nuse_json_prompts(False)\n```\n\n## Environment Configuration\n\n### Using Environment Variables\n\n```bash\n# Enable JSON prompts (default)\nexport USE_JSON_PROMPTS=true\n\n# Disable JSON prompts\nexport USE_JSON_PROMPTS=false\n```\n\n### Using .env File\n\n```env\n# .env file\nUSE_JSON_PROMPTS=true\n```\n\n### Programmatic Control\n\n```python\nimport os\n\n# Set environment variable\nos.environ['USE_JSON_PROMPTS'] = 'true'\n\n# Or use the helper function\nfrom knowledge_base_agent.prompts_replacement import use_json_prompts\nuse_json_prompts(True)\n```\n\n## Complete API Reference\n\n### LLMPrompts Class\n\nAll methods maintain identical signatures to the original `LLMPrompts` class:\n\n#### Chat Methods\n\n```python\n# Basic chat prompt\nchat_prompt = LLMPrompts.get_chat_prompt()\n\n# Context preparation for chat\ncontext_prompt = LLMPrompts.get_chat_context_preparation_prompt()\n\n# Synthesis-aware chat\nsynthesis_chat = LLMPrompts.get_synthesis_aware_chat_prompt()\n\n# Contextual chat response with query type\ncontextual_prompt = LLMPrompts.get_contextual_chat_response_prompt(\"technical\")\n```\n\n#### Categorization Methods\n\n```python\n# Standard categorization\ncategorization_prompt = LLMPrompts.get_categorization_prompt_standard(\n    context_content=\"Advanced React hooks patterns\",\n    formatted_existing_categories=\"frontend_frameworks, react_patterns, state_management\",\n    is_thread=False\n)\n\n# Thread categorization\nthread_categorization = LLMPrompts.get_categorization_prompt_standard(\n    context_content=\"Thread about microservices architecture\",\n    formatted_existing_categories=\"microservices, architecture, distributed_systems\",\n    is_thread=True\n)\n```\n\n#### Knowledge Base Item Generation\n\n```python\n# KB item generation with complex context data\ncontext_data = {\n    'tweet_segments': [\n        {'text': 'First part of the content', 'order': 1},\n        {'text': 'Second part of the content', 'order': 2}\n    ],\n    'tweet_text': 'Complete tweet text',\n    'media_descriptions': ['Screenshot of code', 'Diagram showing architecture'],\n    'urls': ['https://example.com/article'],\n    'is_thread': False\n}\n\nkb_prompt = LLMPrompts.get_kb_item_generation_prompt_standard(context_data)\n```\n\n#### README Generation Methods\n\n```python\n# README introduction\nkb_stats = {'total_items': 150, 'categories': 12, 'synthesis_docs': 8}\ncategory_list = \"frontend_frameworks, backend_systems, devops_tools\"\n\nreadme_intro = LLMPrompts.get_readme_introduction_prompt_standard(kb_stats, category_list)\n\n# Category description\ncategory_desc = LLMPrompts.get_readme_category_description_prompt_standard(\n    main_display=\"Frontend Frameworks\",\n    total_cat_items=25,\n    active_subcats=[\"React Patterns\", \"Vue.js Techniques\", \"Angular Best Practices\"]\n)\n```\n\n#### Synthesis Methods\n\n```python\n# Synthesis generation\nsynthesis_prompt = LLMPrompts.get_synthesis_generation_prompt_standard(\n    main_category=\"frontend_frameworks\",\n    sub_category=\"react_patterns\",\n    kb_items_content=\"Content from multiple KB items...\",\n    synthesis_mode=\"comprehensive\"  # or \"technical_deep_dive\"\n)\n\n# Synthesis markdown generation\nmarkdown_prompt = LLMPrompts.get_synthesis_markdown_generation_prompt_standard(\n    synthesis_json='{\"title\": \"React Patterns\", \"content\": \"...\"}',\n    main_category=\"frontend_frameworks\",\n    sub_category=\"react_patterns\",\n    item_count=15\n)\n\n# Main category synthesis\nmain_synthesis = LLMPrompts.get_main_category_synthesis_prompt()\n```\n\n#### Utility Methods\n\n```python\n# Short name generation\nshort_name_prompt = LLMPrompts.get_short_name_generation_prompt()\n```\n\n### ReasoningPrompts Class\n\nAll methods return dictionaries with 'system' and 'user' keys for reasoning models:\n\n#### System Message\n\n```python\n# Get system message for reasoning models\nsystem_msg = ReasoningPrompts.get_system_message()\n# Returns: {\"system\": \"You are an expert...\", \"user\": \"\"}\n```\n\n#### Categorization\n\n```python\n# Reasoning model categorization\ncategorization = ReasoningPrompts.get_categorization_prompt(\n    context_content=\"Advanced React hooks patterns\",\n    formatted_existing_categories=\"frontend_frameworks, react_patterns\",\n    is_thread=False\n)\n# Returns: {\"system\": \"...\", \"user\": \"...\"}\n```\n\n#### Knowledge Base Item Generation\n\n```python\n# KB item generation for reasoning models\nkb_reasoning = ReasoningPrompts.get_kb_item_generation_prompt(\n    tweet_text=\"Complete tweet content\",\n    categories={\"main_category\": \"frontend_frameworks\", \"sub_category\": \"react_patterns\"},\n    media_descriptions=[\"Code screenshot\", \"Architecture diagram\"]\n)\n# Returns: {\"system\": \"...\", \"user\": \"...\"}\n```\n\n#### README Generation\n\n```python\n# README generation for reasoning models\nreadme_reasoning = ReasoningPrompts.get_readme_generation_prompt(\n    kb_stats={'total_items': 150, 'categories': 12},\n    category_list=\"frontend_frameworks, backend_systems\"\n)\n# Returns: {\"system\": \"...\", \"user\": \"...\"}\n\n# Category description for reasoning models\ncategory_reasoning = ReasoningPrompts.get_readme_category_description_prompt(\n    main_display=\"Frontend Frameworks\",\n    total_cat_items=25,\n    active_subcats=[\"React Patterns\", \"Vue.js Techniques\"]\n)\n# Returns: {\"system\": \"...\", \"user\": \"...\"}\n```\n\n#### Synthesis Methods\n\n```python\n# Synthesis generation for reasoning models\nsynthesis_reasoning = ReasoningPrompts.get_synthesis_generation_prompt(\n    main_category=\"frontend_frameworks\",\n    sub_category=\"react_patterns\",\n    kb_items_content=\"Content from multiple KB items...\",\n    synthesis_mode=\"comprehensive\"\n)\n# Returns: {\"system\": \"...\", \"user\": \"...\"}\n\n# Synthesis markdown for reasoning models\nmarkdown_reasoning = ReasoningPrompts.get_synthesis_markdown_generation_prompt(\n    synthesis_json='{\"title\": \"React Patterns\", \"content\": \"...\"}',\n    main_category=\"frontend_frameworks\",\n    sub_category=\"react_patterns\",\n    item_count=15\n)\n# Returns: {\"system\": \"...\", \"user\": \"...\"}\n```\n\n## System Information and Diagnostics\n\n### Getting System Information\n\n```python\nfrom knowledge_base_agent.prompts_replacement import get_prompt_system_info\n\ninfo = get_prompt_system_info()\nprint(f\"Using JSON prompts: {info['using_json_prompts']}\")\nprint(f\"Environment variable: {info['environment_variable']}\")\nprint(f\"Fallback available: {info['fallback_available']}\")\n\nif info.get('json_prompts_loaded'):\n    print(f\"Available prompt types: {info['available_prompt_types']}\")\n    print(f\"Total prompts: {info['total_prompts']}\")\nelse:\n    print(f\"JSON prompts load error: {info.get('error', 'Unknown error')}\")\n```\n\n### Checking Current Status\n\n```python\nfrom knowledge_base_agent.prompts_replacement import is_using_json_prompts\n\nif is_using_json_prompts():\n    print(\"✅ Currently using JSON prompt system\")\nelse:\n    print(\"ℹ️  Currently using original prompt system\")\n```\n\n## Error Handling and Fallback\n\n### Automatic Fallback\n\nThe system automatically falls back to the original prompts if:\n- JSON prompt system fails to initialize\n- Individual prompt rendering fails\n- Environment variable is set to 'false'\n\n```python\n# This will automatically fallback on errors\ntry:\n    chat_prompt = LLMPrompts.get_chat_prompt()\n    print(f\"Prompt loaded successfully: {len(chat_prompt)} characters\")\nexcept Exception as e:\n    print(f\"Error occurred, but fallback should handle it: {e}\")\n```\n\n### Manual Error Handling\n\n```python\nfrom knowledge_base_agent.prompts_replacement import get_prompt_system_info\n\n# Check system status before using\ninfo = get_prompt_system_info()\n\nif not info.get('json_prompts_loaded', False):\n    print(f\"Warning: JSON prompts not loaded: {info.get('error')}\")\n    print(\"System will use original prompts as fallback\")\n\n# Use prompts normally - fallback is automatic\nchat_prompt = LLMPrompts.get_chat_prompt()\n```\n\n## Migration Scenarios\n\n### Gradual Migration\n\n```python\n# Start with JSON prompts disabled\nuse_json_prompts(False)\n\n# Test original functionality\noriginal_prompt = LLMPrompts.get_chat_prompt()\nprint(f\"Original prompt length: {len(original_prompt)}\")\n\n# Enable JSON prompts\nuse_json_prompts(True)\n\n# Test JSON functionality\njson_prompt = LLMPrompts.get_chat_prompt()\nprint(f\"JSON prompt length: {len(json_prompt)}\")\n\n# Compare results\nif abs(len(original_prompt) - len(json_prompt)) < 100:  # Allow small differences\n    print(\"✅ Prompts are similar, migration looks good\")\nelse:\n    print(\"⚠️  Significant differences detected, review needed\")\n```\n\n### A/B Testing\n\n```python\nimport random\n\ndef get_prompt_with_ab_testing():\n    \"\"\"Randomly choose between JSON and original prompts for A/B testing.\"\"\"\n    use_json = random.choice([True, False])\n    use_json_prompts(use_json)\n    \n    prompt = LLMPrompts.get_chat_prompt()\n    system_type = \"JSON\" if is_using_json_prompts() else \"Original\"\n    \n    return prompt, system_type\n\n# Use in your application\nprompt, system_type = get_prompt_with_ab_testing()\nprint(f\"Using {system_type} prompt system\")\n```\n\n### Production Deployment\n\n```python\n# Production-safe deployment pattern\ndef initialize_prompt_system():\n    \"\"\"Initialize prompt system with proper error handling.\"\"\"\n    try:\n        # Try to enable JSON prompts\n        use_json_prompts(True)\n        \n        # Validate system\n        info = get_prompt_system_info()\n        \n        if info.get('json_prompts_loaded'):\n            print(f\"✅ JSON prompt system initialized with {info['total_prompts']} prompts\")\n            return True\n        else:\n            print(f\"⚠️  JSON prompts failed to load: {info.get('error')}\")\n            print(\"Falling back to original prompt system\")\n            use_json_prompts(False)\n            return False\n            \n    except Exception as e:\n        print(f\"❌ Error initializing prompt system: {e}\")\n        print(\"Using original prompt system as fallback\")\n        use_json_prompts(False)\n        return False\n\n# Initialize at application startup\njson_system_available = initialize_prompt_system()\n\n# Use prompts normally - fallback is handled automatically\nchat_prompt = LLMPrompts.get_chat_prompt()\n```\n\n## Performance Considerations\n\n### Caching\n\nThe JSON prompt system includes automatic caching:\n\n```python\n# First call loads and caches the prompt\nstart_time = time.time()\nchat_prompt1 = LLMPrompts.get_chat_prompt()\nfirst_call_time = time.time() - start_time\n\n# Second call uses cached version\nstart_time = time.time()\nchat_prompt2 = LLMPrompts.get_chat_prompt()\nsecond_call_time = time.time() - start_time\n\nprint(f\"First call: {first_call_time:.4f}s\")\nprint(f\"Second call: {second_call_time:.4f}s\")\nprint(f\"Speedup: {first_call_time / second_call_time:.1f}x\")\n```\n\n### Memory Usage\n\n```python\nimport psutil\nimport os\n\n# Monitor memory usage\nprocess = psutil.Process(os.getpid())\n\n# Before loading prompts\nmemory_before = process.memory_info().rss / 1024 / 1024\n\n# Load multiple prompts\nLLMPrompts.get_chat_prompt()\nLLMPrompts.get_categorization_prompt_standard(\"test\", \"test\", False)\nReasoningPrompts.get_system_message()\n\n# After loading prompts\nmemory_after = process.memory_info().rss / 1024 / 1024\n\nprint(f\"Memory usage: {memory_before:.2f} MB -> {memory_after:.2f} MB\")\nprint(f\"Memory increase: {memory_after - memory_before:.2f} MB\")\n```\n\n## Best Practices\n\n### 1. Environment Configuration\n\n```python\n# Set environment early in application lifecycle\nimport os\nos.environ['USE_JSON_PROMPTS'] = 'true'\n\n# Or use configuration management\nfrom your_config import settings\nuse_json_prompts(settings.USE_JSON_PROMPTS)\n```\n\n### 2. Error Handling\n\n```python\n# Always handle potential errors gracefully\ntry:\n    prompt = LLMPrompts.get_categorization_prompt_standard(\n        context_content,\n        formatted_categories,\n        is_thread\n    )\nexcept Exception as e:\n    logger.error(f\"Prompt generation failed: {e}\")\n    # Fallback is automatic, but you might want to log or handle specially\n    raise\n```\n\n### 3. Testing\n\n```python\n# Test both systems in your test suite\ndef test_prompt_systems():\n    \"\"\"Test both JSON and original prompt systems.\"\"\"\n    \n    # Test original system\n    use_json_prompts(False)\n    original_prompt = LLMPrompts.get_chat_prompt()\n    assert len(original_prompt) > 0\n    \n    # Test JSON system\n    use_json_prompts(True)\n    json_prompt = LLMPrompts.get_chat_prompt()\n    assert len(json_prompt) > 0\n    \n    # Prompts should be similar (allow for minor differences)\n    similarity = calculate_similarity(original_prompt, json_prompt)\n    assert similarity > 0.9, f\"Prompts too different: {similarity}\"\n```\n\n### 4. Monitoring\n\n```python\n# Monitor system status in production\ndef health_check():\n    \"\"\"Check prompt system health.\"\"\"\n    try:\n        info = get_prompt_system_info()\n        \n        health_status = {\n            'using_json_prompts': info['using_json_prompts'],\n            'json_prompts_loaded': info.get('json_prompts_loaded', False),\n            'total_prompts': info.get('total_prompts', 0),\n            'fallback_available': info['fallback_available']\n        }\n        \n        # Test basic functionality\n        chat_prompt = LLMPrompts.get_chat_prompt()\n        health_status['basic_functionality'] = len(chat_prompt) > 0\n        \n        return health_status\n        \n    except Exception as e:\n        return {'error': str(e), 'healthy': False}\n\n# Use in monitoring endpoint\nhealth = health_check()\nif not health.get('healthy', True):\n    alert_monitoring_system(health)\n```\n\n## Conclusion\n\nThe `prompts_replacement.py` module provides a seamless way to migrate to the JSON prompt system while maintaining full backward compatibility. Key benefits:\n\n- **Zero Code Changes**: Drop-in replacement with identical interfaces\n- **Automatic Fallback**: Graceful degradation to original system on errors\n- **Environment Control**: Easy switching between systems\n- **Production Ready**: Comprehensive error handling and monitoring\n- **Performance Optimized**: Automatic caching and efficient loading\n\nThe module is designed for production use with robust error handling, automatic fallback mechanisms, and comprehensive monitoring capabilities.\n"